{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous and Asynchronous"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Synchrounous -\n",
    "Our normal python code is a synchronous code. i.e. all steps executed in a sequence.\n",
    "Example: If one web-page is loading and in middle of it it needs to load images from some other server. So first it will load that images and once that images get loaded then next content on same page will get loaded\n",
    "\n",
    "Asynchrounous -\n",
    "In Asynchrounous programming, Tasks can run in parallel.\n",
    "Example: If one web-page is loading and in middle of it it needs to load images from some other server. So when it's requesting resources from other server, at that time our CPU is IDLE. So during that time other tasks will get started i.e. it will load content of rest of the page and will display image when other server get responded from which we are requesting Image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are different ways to doing things in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Multiple Process"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The most obvious way to use multiple process. i.e. From the terminal, you can start your script two, three, four…ten times and then all the scripts are going to run independently or at the same time. \n",
    "\n",
    "but in this case, its Operatin System responsibility how it share the resources (CPU) with the requests.\n",
    "\n",
    "Alternatively, we can use \"Multiprocessing\" library availabe in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID of main process: 125956\n",
      "ID of process running worker1: 125968\n",
      "ID of process running worker2: 125970\n",
      "ID of process p1: 125968\n",
      "ID of process p2: 125970\n",
      "Both processes finished execution!\n",
      "Process p1 is alive: False\n",
      "Process p2 is alive: False\n"
     ]
    }
   ],
   "source": [
    "# importing the multiprocessing module\n",
    "import multiprocessing\n",
    "import os\n",
    "  \n",
    "def worker1():\n",
    "    # printing process id\n",
    "    print(\"ID of process running worker1: {}\".format(os.getpid()))\n",
    "\n",
    "def worker2():\n",
    "    # printing process id\n",
    "    print(\"ID of process running worker2: {}\".format(os.getpid()))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # printing main program process id\n",
    "    print(\"ID of main process: {}\".format(os.getpid()))\n",
    "  \n",
    "    # creating processes\n",
    "    p1 = multiprocessing.Process(target=worker1)\n",
    "    p2 = multiprocessing.Process(target=worker2)\n",
    "  \n",
    "    # starting processes\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "  \n",
    "    # process IDs\n",
    "    print(\"ID of process p1: {}\".format(p1.pid))\n",
    "    print(\"ID of process p2: {}\".format(p2.pid))\n",
    "  \n",
    "    # wait until processes are finished\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "  \n",
    "    # both processes finished\n",
    "    print(\"Both processes finished execution!\")\n",
    "  \n",
    "    # check if processes are alive\n",
    "    print(\"Process p1 is alive: {}\".format(p1.is_alive()))\n",
    "    print(\"Process p2 is alive: {}\".format(p2.is_alive()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Multiple Threads"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The next way to run multiple things at once is to use threads. A thread is a line of execution, pretty much like a process, but you can have multiple threads in the context of one process and they all share access to common resources. But because of this, it’s difficult to write a threading code. And again, the operating system is doing all the heavy lifting on sharing the CPU, but the global interpreter lock (GIL) allows only one thread to run Python code at a given time even when you have multiple threads running code. So, In CPython, the GIL prevents multi-core concurrency. Basically, you’re running in a single core even though you may have two or four or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cube: 1000\n",
      "Square: 100\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    " \n",
    "def print_cube(num):\n",
    "    \"\"\"\n",
    "    function to print cube of given num\n",
    "    \"\"\"\n",
    "    print(\"Cube: {}\".format(num * num * num))\n",
    "    \n",
    "def print_square(num):\n",
    "    \"\"\"\n",
    "    function to print square of given num\n",
    "    \"\"\"\n",
    "    print(\"Square: {}\".format(num * num))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # creating thread\n",
    "    t1 = threading.Thread(target=print_cube, args=(10,))\n",
    "    t2 = threading.Thread(target=print_square, args=(10,))\n",
    " \n",
    "    # starting thread 1\n",
    "    t1.start()\n",
    "    # starting thread 2\n",
    "    t2.start()\n",
    " \n",
    "    # wait until thread 1 is completely executed\n",
    "    t1.join()\n",
    "    # wait until thread 2 is completely executed\n",
    "    t2.join()\n",
    " \n",
    "    # both threads completely executed\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Coroutines using yield"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Coroutines are generalization of subroutines. They are used for cooperative multitasking where a process voluntarily yield (give away) control periodically or when idle in order to enable multiple applications to be run simultaneously. Coroutines are similar to generators but with few extra methods and slight change in how we use yield statement. Generators produce data for iteration while coroutines can also consume data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching prefix:Dear\n",
      "Dear James\n",
      "Closing coroutine!!\n"
     ]
    }
   ],
   "source": [
    "def print_name(prefix):\n",
    "    print(\"Searching prefix:{}\".format(prefix))\n",
    "    try : \n",
    "        while True:\n",
    "                # yeild used to create coroutine\n",
    "                name = (yield)\n",
    "                if prefix in name:\n",
    "                    print(name)\n",
    "    except GeneratorExit:\n",
    "            print(\"Closing coroutine!!\")\n",
    "\n",
    "corou = print_name(\"Dear\")\n",
    "corou.__next__()\n",
    "corou.send(\"James\")\n",
    "corou.send(\"Dear James\")\n",
    "corou.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Asynchronous Programming"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The fourth way is an asynchronous programming, where the OS is not participating. As far as OS is concerned you’re going to have one process and there’s going to be a single thread within that process, but you’ll be able to do multiple things at once. So, what’s the trick?\n",
    "\n",
    "The answer is asyncio\n",
    "\n",
    "asyncio is the new concurrency module introduced in Python 3.4. It is designed to use coroutines and futures to simplify asynchronous code and make it almost as readable as synchronous code as there are no callbacks.\n",
    "\n",
    "There is always a misconception that - Asynchrouns increase the speed - NO\n",
    "But it increase the throuput i.e. it can handle multiple request at a time, But it will not increase the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let’s take a classical example, \n",
    "\n",
    "a chess exhibition where one of the best chess players competes against a lot of people. And if there are 24 games with 24 people to play with and the chess master plays with all of them synchronically, it’ll take at least 12 hours (taking into account that the average game takes 30 moves, the chess master thinks for 5 seconds to come up with a move and the opponent — for approximately 55 seconds). But using the asynchronous mode gives chess master the opportunity to make a move and leave the opponent thinking while going to the next one and making a move there. This way a move on all 24 games can be done in 2 minutes and all of them can be won in just one hour.\n",
    "So, this is what’s meant when people talk about asynchronous being really fast. It’s this kind of fast. The chess master doesn’t play chess faster, the time is just more optimized and it’s not get wasted on waiting around. This is how it works.\n",
    "\n",
    "In this analogy, the chess master will be our CPU and the idea is that we wanna make sure that the CPU doesn’t wait or waits the least amount of time possible. It’s about always finding something to do.\n",
    "A practical definition of Async is that it’s a style of concurrent programming in which tasks release the CPU during waiting periods, so that other tasks can use it. In Python, there are several ways to achieve concurrency, based on our requirement, code flow, data manipulation, architecture design and use cases we can select any of these methods."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Another Example is -\n",
    "\n",
    "Let suppose you are filling a JUG from a TAP\n",
    "and while filling, you noticed that Your Shoe Laces are open.\n",
    "\n",
    "So, do you need to wait for Jug to Fill, to bind your shoe laces - Really Not...\n",
    "\n",
    "So the same concept, asynchrouns programming use, If CPU is waiting on some other server, I/O from other server, or Database connection. So during that time, CPU is IDLE. So instead of keep waiting the response for that TASK, what CPU will do is start the next task - which is waiting and came back to earlier task later on ; when it get the response from other server for which it was waiting.\n",
    "\n",
    "So in this way, we are maximizing using the CPU. So this is the whole concept of asynchrounous programming. In this way, we can handle multiple user requests. So increase the throuhput.\n",
    "\n",
    "using asyncio moduele, its programmer who will decide when to Suspend the task and when to resume the task. Because we know when it will wait for I/O or for other things.\n",
    "\n",
    "so in asyncio, OS will not decide when to switch to other task, it's programmer who will decide when to switch to other task and came back to earlier task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sumanshu\n"
     ]
    }
   ],
   "source": [
    "def foo():\n",
    "    return\n",
    "\n",
    "foo()\n",
    "print('Sumanshu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above function foo() whether it do anything or not\n",
    "# it will not print (sumanshu) before its completion\n",
    "# So this is a sychrouns programming."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So, we need to make this to asynchrouns -\n",
    "1) we use asyncio (module) - it's an inbuilt module\n",
    "2) we use the 'async' keyword ; we need to use it before 'def' keyword (which tell the python it's a coroutine)\n",
    "3) we use the 'await' keyword\n",
    "4) We need the event-loop ; which is a kind of schedulor and take care of tasks  # But this is basicaly for the low-level programmer \n",
    "\n",
    "Instead of this, we can directly use asyncio.run() - which automatically taken care of even-loop and its closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object foo at 0x7f644040b1c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def foo():\n",
    "    print('Sumanshu')\n",
    "\n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async function will returns a couroutine object, but it will not execute it as a normal python\n",
    "# function.\n",
    "\n",
    "# To actually run this we need to use 'await' command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sumanshu\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def foo():\n",
    "    print('Sumanshu')\n",
    "\n",
    "await foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-769f0816957c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'World'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.8/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    print('Hello')\n",
    "    await asyncio.sleep(1)\n",
    "    print('World')\n",
    "\n",
    "asyncio.run(main())  \n",
    "# This gives error, because we are running in Ipython/Jupyter which is also running one event loop\n",
    "# If we run this as a standalone code, it will work\n",
    "\n",
    "# Check the first.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Using Redis and Redis Queue RQ"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using asyncio and aiohttp may not always be in an option especially if you are using older versions of python. Also, there will be scenarios when you would want to distribute your tasks across different servers. In that case we can leverage RQ (Redis Queue). It is a simple Python library for queueing jobs and processing them in the background with workers. It is backed by Redis — a key/value data store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concurrent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Concurrent - means things occuring at same time. It's opposite of consecutive (Sequential)\n",
    "\n",
    "Concurrency is different than Parallelism\n",
    "\n",
    "Concurrency - asyncio  [Dealing multiple things at once]\n",
    "Parallelism - Threading / MultiProcessing [ Multiple tasks DO simultaneously]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coroutines in Python (async/await)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Coroutines is a simple function which can pause and resume processing at any stage, during its lifecycle\n",
    "\n",
    "To make coroutine, we need below things\n",
    "\n",
    "1- Function\n",
    "2 - async keyword in front of defination of function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object main at 0x7f644040b2c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()  \n",
    "# So, when we call a coroutine, we get coroutine object\n",
    "# but that coroutine did not execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pause the coroutine using 'await' keyword\n",
    "# So as soon as 'await' keyword enconter,\n",
    "# context switch happen to another task\n",
    "\n",
    "async def main():\n",
    "    await awaitable_object\n",
    "    \n",
    "# awaitable_object : can be coroutines, Tasks, Futures\n",
    "# tasks & futures : are defined in a inbuilt library of Python i.e. asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks: are used to schedule coroutine\n",
    "# When a coroutine is wrapped into a task with functions like asyncio.create_task()\n",
    "# the coroutine is automatically schedule to run soon.\n",
    "\n",
    "# Future: is a low-level awaitable object that represents an eventual result of an asynchrounous operation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check third.py\n",
    "\n",
    "# To execute a coroutine, we need an event loop\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(my_coroutine())\n",
    "loop.close()\n",
    "\n",
    "\n",
    "# Instead of above 3 lines, we can use the shortcut as well\n",
    "asyncio.run(my_coroutine())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "\n",
    "# async def main():\n",
    "#     print(\"hello\")\n",
    "#     await asyncio.sleep(2)\n",
    "#     print('World')\n",
    "    \n",
    "# asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concurrent Execution using asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check fifth.py\n",
    "\n",
    "# We can use asyncio.gather - to execute multiple coroutines together\n",
    "\n",
    "# import asyncio\n",
    "\n",
    "# async def main():\n",
    "#     await asyncio.gather(cor1(), cor2(), cor3())\n",
    "    \n",
    "# asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asyncio"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "asyncio is a library to write concurrent code using the async/await syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asyncio provides a set of high-level APIs to:\n",
    "- run Python coroutines concurrently and have full control over their execution;\n",
    "- perform network IO and IPC;\n",
    "- control subprocesses;\n",
    "- distribute tasks via queues;\n",
    "- synchronize concurrent code;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coroutines"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Coroutines declared with the async/await syntax is the preferred way of writing asyncio applications."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For example, the following snippet of code (requires Python 3.7+) prints “hello”, waits 1 second, and then prints “world”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    print(\"Hello\")\n",
    "    await asyncio.sleep(1)\n",
    "    print(\"...World\")\n",
    "    \n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that simply calling a coroutine will not schedule it to be executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object main at 0x7fa5ac5d56c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To actually run a coroutine, asyncio provides three main mechanisms:\n",
    "- asyncio.run() \n",
    "- awaiting on coroutines\n",
    "- asyncio.create_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asyncio.run() - function to run the top-level entry point “main()” function (see the above example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Awaiting on a coroutine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "async def say_after(delay, what):\n",
    "    await asyncio.sleep(delay)\n",
    "    print(what)\n",
    "\n",
    "async def main():\n",
    "    print(f\"started at {time.strftime('%X')}\")\n",
    "    await say_after(1, 'hello')\n",
    "    await say_after(2, 'world')\n",
    "    print(f\"finished at {time.strftime('%X')}\")\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asyncio.create_task() function to run coroutines concurrently as asyncio Tasks.\n",
    "# this code will run faster than above 2 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    task1 = asyncio.create_task(say_after(1, 'hello'))\n",
    "    task2 = asyncio.create_task(say_after(2, 'world'))\n",
    "    print(f\"started at {time.strftime('%X')}\")\n",
    "    # Wait until both tasks are completed (should take\n",
    "    # around 2 seconds.)\n",
    "    await task1\n",
    "    await task2\n",
    "\n",
    "    print(f\"finished at {time.strftime('%X')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awaitables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We say that an object is an awaitable object if it can be used in an await expression. Many asyncio APIs are designed to accept awaitables.\n",
    "\n",
    "There are three main types of awaitable objects: coroutines, Tasks, and Futures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coroutines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def nested():\n",
    "    return 42\n",
    "\n",
    "async def main():\n",
    "    # Nothing happens if we just call \"nested()\".\n",
    "    # A coroutine object is created but not awaited,\n",
    "    # so it *won't run at all*.\n",
    "    nested()\n",
    "\n",
    "    # Let's do it differently now and await it:\n",
    "    print(await nested())  # will print \"42\".\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tasks are used to schedule coroutines concurrently.\n",
    "\n",
    "When a coroutine is wrapped into a Task with functions like asyncio.create_task() the coroutine is automatically scheduled to run soon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def nested():\n",
    "    return 42\n",
    "\n",
    "async def main():\n",
    "    # Schedule nested() to run soon concurrently\n",
    "    # with \"main()\".\n",
    "    task = asyncio.create_task(nested())\n",
    "\n",
    "    # \"task\" can now be used to cancel \"nested()\", or\n",
    "    # can simply be awaited to wait until it is complete:\n",
    "    await task\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Streams are high-level async/await-ready primitives to work with network connections. Streams allow sending and receiving data without using callbacks or low-level protocols and transports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def tcp_echo_client(message):\n",
    "    reader, writer = await asyncio.open_connection(\n",
    "        '127.0.0.1', 8888)\n",
    "\n",
    "    print(f'Send: {message!r}')\n",
    "    writer.write(message.encode())\n",
    "    await writer.drain()\n",
    "\n",
    "    data = await reader.read(100)\n",
    "    print(f'Received: {data.decode()!r}')\n",
    "\n",
    "    print('Close the connection')\n",
    "    writer.close()\n",
    "    await writer.wait_closed()\n",
    "\n",
    "asyncio.run(tcp_echo_client('Hello World!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subprocesses"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This section describes high-level async/await asyncio APIs to create and manage subprocesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run(cmd):\n",
    "    proc = await asyncio.create_subprocess_shell(\n",
    "        cmd,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE)\n",
    "\n",
    "    stdout, stderr = await proc.communicate()\n",
    "\n",
    "    print(f'[{cmd!r} exited with {proc.returncode}]')\n",
    "    if stdout:\n",
    "        print(f'[stdout]\\n{stdout.decode()}')\n",
    "    if stderr:\n",
    "        print(f'[stderr]\\n{stderr.decode()}')\n",
    "\n",
    "asyncio.run(run('ls /zzz'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is wasy to run multiple sub-process in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    await asyncio.gather(\n",
    "        run('ls /zzz'),\n",
    "        run('sleep 1; echo \"hello\"'))\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queues"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "asyncio queues are designed to be similar to classes of the queue module. Although asyncio queues are not thread-safe, they are designed to be used specifically in async/await code.\n",
    "\n",
    "Note that methods of asyncio queues don’t have a timeout parameter; use asyncio.wait_for() function to do queue operations with a timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check queue_1.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
